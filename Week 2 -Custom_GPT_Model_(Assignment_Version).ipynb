{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eo_QP1ITFfX2"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
        "\n",
        "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
      ],
      "metadata": {
        "id": "UWiGVj6njoDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "namJ1XDbZleu",
        "outputId": "5cc50af7-2e29-494f-8cfc-f7ca7156f5f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection\n",
        "\n",
        "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
        "\n",
        "Let's start by grabbing our source repository for the day!"
      ],
      "metadata": {
        "id": "eHi04aEnkKEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "f861c822-bf05-45d9-f9d7-48ac7a0789c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n",
            "Receiving objects: 100% (649/649), 935.48 KiB | 3.64 MiB/s, done.\n",
            "Resolving deltas: 100% (373/373), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to grab some dependencies.\n",
        "\n",
        "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
      ],
      "metadata": {
        "id": "6l4CqoEDl7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_gepPv1Qdj_",
        "outputId": "664b29f6-46bf-480f-c5f6-8db6a88465ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first - let's download our dataset!\n",
        "\n",
        "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set. We want ~90% of our data to be training, and ~10% to be validation."
      ],
      "metadata": {
        "id": "70hSjXmZmCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path = \"/data/shakespeare\"\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "train_data = data[:round(n*0.9)]\n",
        "val_data = data[round(n*0.9):]"
      ],
      "metadata": {
        "id": "T7qRWArUNiZ5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
      ],
      "metadata": {
        "id": "wU9BG2CymU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFnrwKpQPsYh",
        "outputId": "d5a57c6a-3899-46cd-a7b8-5108d1fdb6a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
        "\n",
        "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmWXE5ctma9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is BPE?\n",
        "\n",
        "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
        "\n",
        "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
        "\n",
        "Let's take the following text and break it apart into its word components.\n",
        "\n",
        "\n",
        "```\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "```\n",
        "\n",
        "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
      ],
      "metadata": {
        "id": "GLecDiHbogvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "#Any spaces will be considered as a word\n",
        "naive_word_list = input_text.split()\n",
        "naive_word_list[:10]"
      ],
      "metadata": {
        "id": "m34NDAGCpiz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0554d940-e989-41b2-e122-12eb36c80fa7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['After',\n",
              " 'pre-tokenization,',\n",
              " 'a',\n",
              " 'set',\n",
              " 'of',\n",
              " 'unique',\n",
              " 'words',\n",
              " 'has',\n",
              " 'been',\n",
              " 'created']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can count our words and get their frequency."
      ],
      "metadata": {
        "id": "hR8k-2bopqjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "e74517e5-ad83-4ca4-99eb-82c640699c87"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
      ],
      "metadata": {
        "id": "NckufSxxp-w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return len(vocab)"
      ],
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3kCf-WvdBL",
        "outputId": "924948a7-130e-44f9-f55b-80200fa24e73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are ~34 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
      ],
      "metadata": {
        "id": "VoMq7GhKqf7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
      ],
      "metadata": {
        "id": "OGxrHYmftDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "0954da60-a3a6-4a70-ab84-0850096ac701"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
        "\n",
        "Let's see how this process looks in code."
      ],
      "metadata": {
        "id": "OqORqdzwsZ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ],
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ],
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0XtvLbpxbSx",
        "outputId": "2af352b5-b894-4898-c8e1-3c1b67cc0573"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After one merge, we can see that `t h` has been converted to `th`!\n",
        "\n",
        "Let's see how that impacted our vocabulary."
      ],
      "metadata": {
        "id": "9DPkBzj2u-me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(new_vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "a92c45a4-93d8-48d1-80cf-ebf600828d65"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
        "\n",
        "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
      ],
      "metadata": {
        "id": "o3M13D60xzZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
        "\n",
        "Let's walk through the steps we'll take:\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
        "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
        "\n",
        "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
        "\n",
        "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
        "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
      ],
      "metadata": {
        "id": "BePYCbHly02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([\n",
        "    Sequence([NFD()]) #Just a way to normalize our tokens\n",
        "])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
        "\n",
        "Let's use the following:\n",
        "\n",
        "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
        "- `\"</s>\"`   : eos_token - end of sequence token\n",
        "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
        "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
        "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
        "\n",
        "We're also going to set a target vocabulary of 50,000 tokens."
      ],
      "metadata": {
        "id": "dDqkNNdM1KsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "      \"<s>\",\n",
        "      \"<pad>\",\n",
        "       \"</s>\",\n",
        "      \"<unk>\",\n",
        "      \"<mast>\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing left to do but point it at our data-source and let it train!\n",
        "\n",
        "We'll use the `.train()` method to accomplish this task.\n",
        "\n",
        "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
        "\n",
        "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
      ],
      "metadata": {
        "id": "yQ8X9vZe2Fyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(\n",
        "    files=[input_file_path], trainer=trainer\n",
        ")"
      ],
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ],
      "metadata": {
        "id": "V2JNYiqB2qKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/tokenizer'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "4f17ceb4-5167-44aa-d7a7-03e308aafdd1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOlbggdRFrN",
        "outputId": "7eb9d959-0966-438d-9c32-3cafca51d9b2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "#save path is pointing to tokenizer we pretrained!!!\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it tokenizes our inputs!"
      ],
      "metadata": {
        "id": "0-Bnq7lV2xWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHY5VufRbBj",
        "outputId": "2fd4617e-c88f-4646-9d9e-6329b1fd7cf4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hark',\n",
              " ',',\n",
              " 'Ġmy',\n",
              " 'Ġname',\n",
              " 'Ġbe',\n",
              " 'ĠRomeo',\n",
              " '!',\n",
              " 'ĠI',\n",
              " 'Ġam',\n",
              " 'Ġbut',\n",
              " 'Ġa',\n",
              " 'Ġbeautiful',\n",
              " 'Ġsummer',\n",
              " \"'s\",\n",
              " 'Ġday',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "encoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "4880b6d5-2628-41b3-8eff-433deff5ddc6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "83634011-7015-47c2-c09c-6049703a9958"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean up space doesn't do much!\n",
        "tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qofku6_66H7U",
        "outputId": "6af8316c-819f-4b9a-8405-4742403fe035"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
        "\n",
        "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
        "\n",
        "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
      ],
      "metadata": {
        "id": "ji3sF-rA21YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = tokenizer.encode(train_data)\n",
        "val_ids = tokenizer.encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "940e633a-4996-40a0-9b1a-d73f37240d7f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 291,285 tokens\n",
            "val has 34,222 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ],
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training The Model\n",
        "\n",
        "Now that we have our tokenized dataset, let's get to training our model!\n",
        "\n",
        "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
        "\n",
        "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
      ],
      "metadata": {
        "id": "c0I3VrRC3XIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "423d757c-5c7c-447b-bf9f-4a6e3fd8f9c1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do some critical imports."
      ],
      "metadata": {
        "id": "13p1e8sa3k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameters\n",
        "\n",
        "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
      ],
      "metadata": {
        "id": "kY_vWZG-3uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I/O\n",
        "\n",
        "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
      ],
      "metadata": {
        "id": "OykCjVQK5EX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out'"
      ],
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization\n",
        "\n",
        "Since we're training from scratch, we'll use `init_from = 'scratch'`."
      ],
      "metadata": {
        "id": "A5iwwrNL5H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'"
      ],
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval and Logging\n",
        "\n",
        "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
        "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
        "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
        "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
        "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
      ],
      "metadata": {
        "id": "2YlolKOj4_dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 250 #how often do we want to evaluate our model on validation set while training!\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ],
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "We can set our dataset here - we'll use the one we created earlier!"
      ],
      "metadata": {
        "id": "a488zaF_4zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'shakespeare'"
      ],
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Typical Hyper-Parameters\n",
        "\n",
        "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
        "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
        "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
      ],
      "metadata": {
        "id": "XP9rBgGc426Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1 #for simulating large batch size and concat them together?\n",
        "batch_size = 16\n",
        "block_size =512 #input block size\n"
      ],
      "metadata": {
        "id": "EM_ybLPP43Pd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture\n",
        "\n",
        "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
        "- `n_head` - this is the number of attention heads in each decoder layer!\n",
        "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook. A default value of ~`500` should do the trick!\n",
        "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
        "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers."
      ],
      "metadata": {
        "id": "UZ-8bDIY45GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 6\n",
        "n_head = 8 # Changing from 6 to 8 to avoid config.n_embd % config.n_head ==0 assertion error\n",
        "n_embd = 512 #model_d\n",
        "dropout = 0.2\n",
        "bias = False"
      ],
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ QUESTION:\n",
        "\n",
        "What condition must be true as it relates to the `n_embd` and `n_head`?"
      ],
      "metadata": {
        "id": "piNHkSaRNDjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer - The condition that must be true, because of which an assertion error was thrown is that n_embd % n_head ==0. This is assuming that all the embeddings will be used by the heads (either sequentially or parallely)\n",
        "and the distribution has to be aligned"
      ],
      "metadata": {
        "id": "A6LEeFpwIhUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Hyper-Parameters\n",
        "\n",
        "Basic Optimizer Hyper-Parameters:\n",
        "\n",
        "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
        "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
        "\n",
        "Learning Rate Decay Settings:\n",
        "\n",
        "- `decay_lr` - set decay flag\n",
        "- `weight_Decay` - how much to decay lr by\n",
        "- `lr_decay_iters` - should be set to ~max_iters.\n",
        "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
        "\n",
        "Clipping and Warmup:\n",
        "\n",
        "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
        "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
        "\n",
        "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
      ],
      "metadata": {
        "id": "3NWDTaAz7gwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 1e-1\n",
        "lr_decay_iters = 5000 # ~= max_iters per Chinchilla\n",
        "min_lr = 1e-4 # ~= learning_rate/10 per Chinchilla\n",
        "\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ QUESTION:\n",
        "\n",
        "Given a Learning Rate of `1e-4` and a maximum iteration cap of `10,000`: What should `lr_decay_iters` be, and what should `min_lr` be?"
      ],
      "metadata": {
        "id": "AzHvpMDTNfU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the `lr_decay_iters` can be equal to the max iterations or lessers, depending upon if we want the learning rate to decay at every iteration or not"
      ],
      "metadata": {
        "id": "joSc5JJUIrAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
      ],
      "metadata": {
        "id": "ucldc4mz9yeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "print(dtype)\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "34e35438-ade1-4164-a108-db4ba7e66c43"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float16\n",
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Settings\n",
        "\n",
        "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
        "\n",
        "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
      ],
      "metadata": {
        "id": "eKmdfbye-BNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "This block will:\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
      ],
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question:\n",
        "\n",
        "What can you tell us about the way the labels are generated?\n",
        "\n",
        "Please produce an example of a single x and y pair."
      ],
      "metadata": {
        "id": "I-tifZVD-9hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample of X and Y\n",
        "data = train_data\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "sample_index = np.random.randint(1,len(data))\n",
        "sample_len = 50\n",
        "print(f\"*****Sample X at index {sample_index}\\n with shape {x[0].shape} : {tokenizer.decode(x[0][:sample_len])}\")\n",
        "print(f\"\\n*****Sample Y at index {sample_index}\\n with shape {y[0].shape}: {tokenizer.decode(y[0][:sample_len])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWhSJXQxJCNR",
        "outputId": "6e8f15ac-f7cd-44d4-e786-8d12ed9a7f0e"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Sample X at index 211513\n",
            " with shape torch.Size([512]) : PRINCE EDWARD:\n",
            "I know my duty; you are all undutiful:\n",
            "Lascivious Edward, and thou perjured George,\n",
            "And thou mis-shapen Dick, I tell ye all\n",
            "I am your better, traitors as ye are:\n",
            "\n",
            "*****Sample Y at index 211513\n",
            " with shape torch.Size([512]): RINCE EDWARD:\n",
            "I know my duty; you are all undutiful:\n",
            "Lascivious Edward, and thou perjured George,\n",
            "And thou mis-shapen Dick, I tell ye all\n",
            "I am your better, traitors as ye are:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There for X and Y is simply sequences of text by an offset of 1, with size of the block size. Basically we are training a model with `x`, to predict the next sequence of sentence `y`"
      ],
      "metadata": {
        "id": "xTZnMnEJKzma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Initialization of Model\n",
        "\n",
        "Here we init our number of iterations as 0, and our best val loss as a very high number."
      ],
      "metadata": {
        "id": "EbDlW-68_atH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ],
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain our vocab size from our trained tokenizer."
      ],
      "metadata": {
        "id": "A4Uj9qBI_vXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "567cca91-1974-44f2-d6c0-af6191988f0c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our model args dict.\n",
        "\n",
        "Use the following as a guide: [Here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L109)"
      ],
      "metadata": {
        "id": "V7bcNelYARmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(\n",
        "    n_layer= n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    vocab_size=None,\n",
        "    dropout=dropout\n",
        ")"
      ],
      "metadata": {
        "id": "JfIWEbanV7ZS"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd % n_head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOfLZ0sg_uaF",
        "outputId": "20408c06-f51a-4db9-ffd3-14069c61e061"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our model with the provided `model_args`.\n",
        "\n",
        "These are derived from the hyper-parameters we set above."
      ],
      "metadata": {
        "id": "2WWcbkiCAUI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "c65b862e-f5c6-4b49-890e-9b43385e2c48"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 29.17M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
        "\n",
        "Let's set our block_size to the correct size as determined in our configuration steps."
      ],
      "metadata": {
        "id": "BpViOsxLAl6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ],
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at our model in all its glory!"
      ],
      "metadata": {
        "id": "eRgguPLKAuZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaE3KSTnAtJs",
        "outputId": "b45964a9-1d28-49ef-ec75-a194198a26ec"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20099, 512)\n",
              "    (wpe): Embedding(512, 512)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
              "          (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=20099, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
      ],
      "metadata": {
        "id": "LzoEY6gcBOSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
      ],
      "metadata": {
        "id": "6Zs5Hcf9BBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "83e40058-c381-407a-ccfd-b738618efee1"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 29,427,200 parameters\n",
            "num non-decayed parameter tensors: 13, with 6,656 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile our model!\n",
        "\n",
        "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
        "\n",
        "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
      ],
      "metadata": {
        "id": "ZF5YWJoKB4og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FNU0T0WXdI",
        "outputId": "ce01b857-c275-4a83-91ba-1ddf217a4946"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
        "\n",
        "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
      ],
      "metadata": {
        "id": "p6lRcVsZCXRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
        "\n",
        "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n",
        "\n",
        "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
      ],
      "metadata": {
        "id": "fLsOpaACDDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set some specific values in our env to allow training in Colab."
      ],
      "metadata": {
        "id": "cqFePCZmE1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "4b47707f-042a-4a0d-c019-a1fc5faf278f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "\n",
        "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
      ],
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHbyEapRWmpc",
        "outputId": "faebd1f9-7b11-407d-8ef0-ec8791e60ef0"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.0077, val loss 9.9965\n",
            "iter 0: loss 10.0217, time 73385.27ms, mfu -100.00%\n",
            "iter 10: loss 8.4180, time 152.75ms, mfu 3.33%\n",
            "iter 20: loss 7.4989, time 151.17ms, mfu 3.34%\n",
            "iter 30: loss 6.3675, time 151.53ms, mfu 3.34%\n",
            "iter 40: loss 5.8192, time 152.27ms, mfu 3.34%\n",
            "iter 50: loss 5.6151, time 153.29ms, mfu 3.34%\n",
            "iter 60: loss 5.4525, time 150.06ms, mfu 3.34%\n",
            "iter 70: loss 5.2471, time 151.84ms, mfu 3.34%\n",
            "iter 80: loss 4.9533, time 152.26ms, mfu 3.34%\n",
            "iter 90: loss 4.8705, time 150.27ms, mfu 3.35%\n",
            "iter 100: loss 4.7093, time 152.74ms, mfu 3.35%\n",
            "iter 110: loss 4.5810, time 153.30ms, mfu 3.34%\n",
            "iter 120: loss 4.5843, time 151.95ms, mfu 3.34%\n",
            "iter 130: loss 4.4988, time 150.39ms, mfu 3.35%\n",
            "iter 140: loss 4.6286, time 151.64ms, mfu 3.35%\n",
            "iter 150: loss 4.3676, time 152.97ms, mfu 3.35%\n",
            "iter 160: loss 4.4000, time 151.67ms, mfu 3.35%\n",
            "iter 170: loss 4.3794, time 153.55ms, mfu 3.35%\n",
            "iter 180: loss 4.2318, time 153.70ms, mfu 3.34%\n",
            "iter 190: loss 4.2527, time 153.85ms, mfu 3.34%\n",
            "iter 200: loss 4.1059, time 153.03ms, mfu 3.34%\n",
            "iter 210: loss 4.2040, time 154.36ms, mfu 3.33%\n",
            "iter 220: loss 4.2393, time 153.58ms, mfu 3.33%\n",
            "iter 230: loss 4.0408, time 153.25ms, mfu 3.33%\n",
            "iter 240: loss 3.9827, time 155.49ms, mfu 3.33%\n",
            "step 250: train loss 3.9934, val loss 4.9168\n",
            "saving checkpoint to out\n",
            "iter 250: loss 4.2300, time 15806.48ms, mfu 3.00%\n",
            "iter 260: loss 4.1131, time 158.92ms, mfu 3.02%\n",
            "iter 270: loss 4.0146, time 155.63ms, mfu 3.04%\n",
            "iter 280: loss 3.9719, time 155.42ms, mfu 3.07%\n",
            "iter 290: loss 3.9815, time 154.29ms, mfu 3.09%\n",
            "iter 300: loss 4.0165, time 156.45ms, mfu 3.11%\n",
            "iter 310: loss 3.8145, time 156.39ms, mfu 3.12%\n",
            "iter 320: loss 3.7719, time 158.63ms, mfu 3.13%\n",
            "iter 330: loss 3.7746, time 157.89ms, mfu 3.14%\n",
            "iter 340: loss 3.7636, time 157.07ms, mfu 3.15%\n",
            "iter 350: loss 3.9366, time 157.64ms, mfu 3.16%\n",
            "iter 360: loss 3.7124, time 156.06ms, mfu 3.17%\n",
            "iter 370: loss 3.7607, time 158.88ms, mfu 3.17%\n",
            "iter 380: loss 3.7526, time 155.47ms, mfu 3.18%\n",
            "iter 390: loss 3.6387, time 156.25ms, mfu 3.19%\n",
            "iter 400: loss 3.4768, time 157.66ms, mfu 3.19%\n",
            "iter 410: loss 3.5760, time 160.35ms, mfu 3.19%\n",
            "iter 420: loss 3.6691, time 157.90ms, mfu 3.19%\n",
            "iter 430: loss 3.6940, time 155.54ms, mfu 3.20%\n",
            "iter 440: loss 3.5743, time 160.09ms, mfu 3.20%\n",
            "iter 450: loss 3.6925, time 157.58ms, mfu 3.20%\n",
            "iter 460: loss 3.5291, time 158.38ms, mfu 3.20%\n",
            "iter 470: loss 3.6447, time 160.81ms, mfu 3.20%\n",
            "iter 480: loss 3.4078, time 156.25ms, mfu 3.21%\n",
            "iter 490: loss 3.4170, time 159.41ms, mfu 3.21%\n",
            "step 500: train loss 3.3415, val loss 5.1472\n",
            "saving checkpoint to out\n",
            "iter 500: loss 3.4167, time 16434.66ms, mfu 2.89%\n",
            "iter 510: loss 3.3686, time 161.00ms, mfu 2.92%\n",
            "iter 520: loss 3.4333, time 160.47ms, mfu 2.94%\n",
            "iter 530: loss 3.3653, time 162.16ms, mfu 2.96%\n",
            "iter 540: loss 3.3172, time 161.10ms, mfu 2.98%\n",
            "iter 550: loss 3.2424, time 160.87ms, mfu 3.00%\n",
            "iter 560: loss 3.2594, time 158.91ms, mfu 3.02%\n",
            "iter 570: loss 3.0318, time 159.22ms, mfu 3.04%\n",
            "iter 580: loss 3.1654, time 160.40ms, mfu 3.05%\n",
            "iter 590: loss 3.3390, time 160.59ms, mfu 3.06%\n",
            "iter 600: loss 3.2280, time 159.10ms, mfu 3.08%\n",
            "iter 610: loss 3.1622, time 160.38ms, mfu 3.09%\n",
            "iter 620: loss 3.0830, time 159.68ms, mfu 3.10%\n",
            "iter 630: loss 3.0482, time 159.12ms, mfu 3.11%\n",
            "iter 640: loss 2.9922, time 159.27ms, mfu 3.12%\n",
            "iter 650: loss 2.9508, time 160.43ms, mfu 3.12%\n",
            "iter 660: loss 2.8970, time 158.95ms, mfu 3.13%\n",
            "iter 670: loss 3.1655, time 157.45ms, mfu 3.14%\n",
            "iter 680: loss 2.9826, time 159.58ms, mfu 3.15%\n",
            "iter 690: loss 2.9324, time 159.02ms, mfu 3.15%\n",
            "iter 700: loss 2.9293, time 160.88ms, mfu 3.15%\n",
            "iter 710: loss 2.8404, time 159.99ms, mfu 3.16%\n",
            "iter 720: loss 2.9959, time 159.88ms, mfu 3.16%\n",
            "iter 730: loss 3.1297, time 158.59ms, mfu 3.16%\n",
            "iter 740: loss 2.7321, time 158.10ms, mfu 3.17%\n",
            "step 750: train loss 2.6428, val loss 5.4422\n",
            "saving checkpoint to out\n",
            "iter 750: loss 2.9175, time 16432.32ms, mfu 2.86%\n",
            "iter 760: loss 3.0364, time 159.33ms, mfu 2.89%\n",
            "iter 770: loss 2.8494, time 160.59ms, mfu 2.92%\n",
            "iter 780: loss 2.8002, time 159.65ms, mfu 2.94%\n",
            "iter 790: loss 2.5841, time 158.08ms, mfu 2.97%\n",
            "iter 800: loss 2.8878, time 160.46ms, mfu 2.99%\n",
            "iter 810: loss 2.6992, time 159.60ms, mfu 3.01%\n",
            "iter 820: loss 2.7499, time 158.71ms, mfu 3.03%\n",
            "iter 830: loss 2.7592, time 159.69ms, mfu 3.05%\n",
            "iter 840: loss 2.5858, time 157.54ms, mfu 3.07%\n",
            "iter 850: loss 2.6727, time 160.09ms, mfu 3.08%\n",
            "iter 860: loss 2.5890, time 158.75ms, mfu 3.09%\n",
            "iter 870: loss 2.6761, time 161.38ms, mfu 3.10%\n",
            "iter 880: loss 2.4684, time 159.22ms, mfu 3.11%\n",
            "iter 890: loss 2.4409, time 155.90ms, mfu 3.12%\n",
            "iter 900: loss 2.7052, time 158.89ms, mfu 3.13%\n",
            "iter 910: loss 2.3250, time 158.20ms, mfu 3.14%\n",
            "iter 920: loss 2.3459, time 159.70ms, mfu 3.14%\n",
            "iter 930: loss 2.4254, time 158.21ms, mfu 3.15%\n",
            "iter 940: loss 2.2295, time 159.58ms, mfu 3.16%\n",
            "iter 950: loss 2.2824, time 160.15ms, mfu 3.16%\n",
            "iter 960: loss 2.3592, time 159.49ms, mfu 3.16%\n",
            "iter 970: loss 2.0855, time 160.12ms, mfu 3.16%\n",
            "iter 980: loss 2.1994, time 162.46ms, mfu 3.16%\n",
            "iter 990: loss 2.0841, time 161.24ms, mfu 3.16%\n",
            "step 1000: train loss 1.9066, val loss 5.7963\n",
            "saving checkpoint to out\n",
            "iter 1000: loss 2.1135, time 16763.23ms, mfu 2.85%\n",
            "iter 1010: loss 2.2421, time 160.97ms, mfu 2.88%\n",
            "iter 1020: loss 2.0014, time 160.88ms, mfu 2.91%\n",
            "iter 1030: loss 2.1397, time 159.25ms, mfu 2.94%\n",
            "iter 1040: loss 1.9698, time 159.50ms, mfu 2.96%\n",
            "iter 1050: loss 2.1942, time 160.83ms, mfu 2.98%\n",
            "iter 1060: loss 2.1077, time 159.18ms, mfu 3.00%\n",
            "iter 1070: loss 2.1252, time 158.93ms, mfu 3.02%\n",
            "iter 1080: loss 2.1458, time 159.99ms, mfu 3.04%\n",
            "iter 1090: loss 2.0165, time 158.97ms, mfu 3.06%\n",
            "iter 1100: loss 1.8600, time 157.81ms, mfu 3.07%\n",
            "iter 1110: loss 2.0360, time 162.02ms, mfu 3.08%\n",
            "iter 1120: loss 1.9334, time 159.84ms, mfu 3.09%\n",
            "iter 1130: loss 1.8568, time 157.96ms, mfu 3.10%\n",
            "iter 1140: loss 1.8418, time 158.60ms, mfu 3.11%\n",
            "iter 1150: loss 1.6993, time 159.54ms, mfu 3.12%\n",
            "iter 1160: loss 1.8335, time 158.97ms, mfu 3.13%\n",
            "iter 1170: loss 1.8267, time 160.11ms, mfu 3.14%\n",
            "iter 1180: loss 1.8620, time 161.20ms, mfu 3.14%\n",
            "iter 1190: loss 1.6159, time 159.35ms, mfu 3.14%\n",
            "iter 1200: loss 1.7021, time 158.00ms, mfu 3.15%\n",
            "iter 1210: loss 1.6675, time 157.90ms, mfu 3.16%\n",
            "iter 1220: loss 1.7536, time 158.91ms, mfu 3.16%\n",
            "iter 1230: loss 1.7516, time 161.07ms, mfu 3.16%\n",
            "iter 1240: loss 1.6540, time 156.61ms, mfu 3.17%\n",
            "step 1250: train loss 1.2518, val loss 6.3370\n",
            "saving checkpoint to out\n",
            "iter 1250: loss 1.7300, time 16465.35ms, mfu 2.86%\n",
            "iter 1260: loss 1.6263, time 160.10ms, mfu 2.89%\n",
            "iter 1270: loss 1.5894, time 158.99ms, mfu 2.92%\n",
            "iter 1280: loss 1.6281, time 155.89ms, mfu 2.96%\n",
            "iter 1290: loss 1.5928, time 159.15ms, mfu 2.98%\n",
            "iter 1300: loss 1.5010, time 161.18ms, mfu 3.00%\n",
            "iter 1310: loss 1.6251, time 158.52ms, mfu 3.02%\n",
            "iter 1320: loss 1.6225, time 158.98ms, mfu 3.04%\n",
            "iter 1330: loss 1.5399, time 161.46ms, mfu 3.05%\n",
            "iter 1340: loss 1.3937, time 159.08ms, mfu 3.06%\n",
            "iter 1350: loss 1.5751, time 157.56ms, mfu 3.08%\n",
            "iter 1360: loss 1.4006, time 159.46ms, mfu 3.09%\n",
            "iter 1370: loss 1.4945, time 158.82ms, mfu 3.10%\n",
            "iter 1380: loss 1.5594, time 159.18ms, mfu 3.11%\n",
            "iter 1390: loss 1.4462, time 159.43ms, mfu 3.12%\n",
            "iter 1400: loss 1.3792, time 160.98ms, mfu 3.13%\n",
            "iter 1410: loss 1.3741, time 161.66ms, mfu 3.13%\n",
            "iter 1420: loss 1.3721, time 157.36ms, mfu 3.14%\n",
            "iter 1430: loss 1.5779, time 160.19ms, mfu 3.14%\n",
            "iter 1440: loss 1.3560, time 158.97ms, mfu 3.15%\n",
            "iter 1450: loss 1.4196, time 160.56ms, mfu 3.15%\n",
            "iter 1460: loss 1.2770, time 160.45ms, mfu 3.15%\n",
            "iter 1470: loss 1.3011, time 160.69ms, mfu 3.15%\n",
            "iter 1480: loss 1.3456, time 160.04ms, mfu 3.16%\n",
            "iter 1490: loss 1.2024, time 158.49ms, mfu 3.16%\n",
            "step 1500: train loss 0.8664, val loss 6.8495\n",
            "saving checkpoint to out\n",
            "iter 1500: loss 1.1465, time 16578.41ms, mfu 2.85%\n",
            "iter 1510: loss 1.2404, time 160.84ms, mfu 2.88%\n",
            "iter 1520: loss 1.2058, time 161.89ms, mfu 2.91%\n",
            "iter 1530: loss 1.1884, time 159.04ms, mfu 2.94%\n",
            "iter 1540: loss 1.2825, time 159.34ms, mfu 2.96%\n",
            "iter 1550: loss 1.2325, time 158.65ms, mfu 2.99%\n",
            "iter 1560: loss 1.1922, time 157.53ms, mfu 3.01%\n",
            "iter 1570: loss 1.1605, time 158.53ms, mfu 3.03%\n",
            "iter 1580: loss 1.1833, time 160.76ms, mfu 3.05%\n",
            "iter 1590: loss 1.1733, time 158.66ms, mfu 3.06%\n",
            "iter 1600: loss 1.1289, time 160.86ms, mfu 3.07%\n",
            "iter 1610: loss 1.2256, time 160.07ms, mfu 3.08%\n",
            "iter 1620: loss 1.1531, time 159.64ms, mfu 3.09%\n",
            "iter 1630: loss 1.1403, time 158.82ms, mfu 3.10%\n",
            "iter 1640: loss 1.1900, time 157.48ms, mfu 3.12%\n",
            "iter 1650: loss 1.1109, time 158.70ms, mfu 3.13%\n",
            "iter 1660: loss 1.0139, time 157.70ms, mfu 3.14%\n",
            "iter 1670: loss 1.1778, time 158.99ms, mfu 3.14%\n",
            "iter 1680: loss 1.1291, time 157.34ms, mfu 3.15%\n",
            "iter 1690: loss 1.1754, time 160.66ms, mfu 3.15%\n",
            "iter 1700: loss 1.1624, time 159.73ms, mfu 3.16%\n",
            "iter 1710: loss 1.1186, time 161.89ms, mfu 3.16%\n",
            "iter 1720: loss 1.1149, time 160.24ms, mfu 3.16%\n",
            "iter 1730: loss 1.0994, time 161.00ms, mfu 3.16%\n",
            "iter 1740: loss 1.0602, time 159.10ms, mfu 3.16%\n",
            "step 1750: train loss 0.6266, val loss 7.1846\n",
            "saving checkpoint to out\n",
            "iter 1750: loss 1.0456, time 16260.02ms, mfu 2.85%\n",
            "iter 1760: loss 0.9686, time 158.98ms, mfu 2.89%\n",
            "iter 1770: loss 1.1136, time 158.73ms, mfu 2.92%\n",
            "iter 1780: loss 1.0693, time 157.99ms, mfu 2.95%\n",
            "iter 1790: loss 1.0394, time 158.41ms, mfu 2.97%\n",
            "iter 1800: loss 0.9914, time 161.39ms, mfu 2.99%\n",
            "iter 1810: loss 1.0899, time 159.77ms, mfu 3.01%\n",
            "iter 1820: loss 0.9368, time 157.42ms, mfu 3.03%\n",
            "iter 1830: loss 0.9776, time 160.20ms, mfu 3.05%\n",
            "iter 1840: loss 0.9687, time 159.24ms, mfu 3.06%\n",
            "iter 1850: loss 0.9586, time 159.68ms, mfu 3.08%\n",
            "iter 1860: loss 0.9370, time 158.31ms, mfu 3.09%\n",
            "iter 1870: loss 0.9277, time 159.73ms, mfu 3.10%\n",
            "iter 1880: loss 0.9674, time 159.60ms, mfu 3.11%\n",
            "iter 1890: loss 0.9609, time 158.86ms, mfu 3.12%\n",
            "iter 1900: loss 0.9530, time 161.93ms, mfu 3.12%\n",
            "iter 1910: loss 0.9362, time 158.59ms, mfu 3.13%\n",
            "iter 1920: loss 0.9260, time 159.80ms, mfu 3.14%\n",
            "iter 1930: loss 0.8577, time 158.46ms, mfu 3.14%\n",
            "iter 1940: loss 0.9056, time 158.50ms, mfu 3.15%\n",
            "iter 1950: loss 0.8834, time 159.54ms, mfu 3.15%\n",
            "iter 1960: loss 0.8139, time 161.66ms, mfu 3.15%\n",
            "iter 1970: loss 0.9035, time 156.82ms, mfu 3.16%\n",
            "iter 1980: loss 0.8613, time 160.37ms, mfu 3.16%\n",
            "iter 1990: loss 0.7937, time 159.93ms, mfu 3.17%\n",
            "step 2000: train loss 0.4701, val loss 7.5802\n",
            "saving checkpoint to out\n",
            "iter 2000: loss 0.7947, time 16760.36ms, mfu 2.85%\n",
            "iter 2010: loss 0.8495, time 160.71ms, mfu 2.88%\n",
            "iter 2020: loss 0.8922, time 158.10ms, mfu 2.92%\n",
            "iter 2030: loss 0.8305, time 160.03ms, mfu 2.94%\n",
            "iter 2040: loss 0.8176, time 159.72ms, mfu 2.97%\n",
            "iter 2050: loss 0.8237, time 158.26ms, mfu 2.99%\n",
            "iter 2060: loss 0.7687, time 158.14ms, mfu 3.02%\n",
            "iter 2070: loss 0.8295, time 160.02ms, mfu 3.03%\n",
            "iter 2080: loss 0.7660, time 159.24ms, mfu 3.05%\n",
            "iter 2090: loss 0.7291, time 156.74ms, mfu 3.07%\n",
            "iter 2100: loss 0.7636, time 159.42ms, mfu 3.08%\n",
            "iter 2110: loss 0.8088, time 158.82ms, mfu 3.09%\n",
            "iter 2120: loss 0.7884, time 160.48ms, mfu 3.10%\n",
            "iter 2130: loss 0.7156, time 158.77ms, mfu 3.11%\n",
            "iter 2140: loss 0.7765, time 159.17ms, mfu 3.12%\n",
            "iter 2150: loss 0.7507, time 158.40ms, mfu 3.13%\n",
            "iter 2160: loss 0.7845, time 156.71ms, mfu 3.14%\n",
            "iter 2170: loss 0.7752, time 157.46ms, mfu 3.15%\n",
            "iter 2180: loss 0.7567, time 159.66ms, mfu 3.15%\n",
            "iter 2190: loss 0.7067, time 159.51ms, mfu 3.16%\n",
            "iter 2200: loss 0.8326, time 160.84ms, mfu 3.16%\n",
            "iter 2210: loss 0.7663, time 157.20ms, mfu 3.17%\n",
            "iter 2220: loss 0.7546, time 161.04ms, mfu 3.17%\n",
            "iter 2230: loss 0.7667, time 160.39ms, mfu 3.17%\n",
            "iter 2240: loss 0.7161, time 160.41ms, mfu 3.17%\n",
            "step 2250: train loss 0.3558, val loss 7.7483\n",
            "saving checkpoint to out\n",
            "iter 2250: loss 0.6799, time 16367.20ms, mfu 2.85%\n",
            "iter 2260: loss 0.7203, time 158.29ms, mfu 2.89%\n",
            "iter 2270: loss 0.7187, time 159.15ms, mfu 2.92%\n",
            "iter 2280: loss 0.7112, time 159.65ms, mfu 2.95%\n",
            "iter 2290: loss 0.6793, time 160.97ms, mfu 2.97%\n",
            "iter 2300: loss 0.6565, time 158.30ms, mfu 2.99%\n",
            "iter 2310: loss 0.6748, time 159.96ms, mfu 3.01%\n",
            "iter 2320: loss 0.6774, time 158.09ms, mfu 3.03%\n",
            "iter 2330: loss 0.6509, time 162.09ms, mfu 3.04%\n",
            "iter 2340: loss 0.6741, time 158.36ms, mfu 3.06%\n",
            "iter 2350: loss 0.6444, time 160.81ms, mfu 3.07%\n",
            "iter 2360: loss 0.7195, time 160.53ms, mfu 3.08%\n",
            "iter 2370: loss 0.6185, time 158.99ms, mfu 3.09%\n",
            "iter 2380: loss 0.7059, time 157.52ms, mfu 3.11%\n",
            "iter 2390: loss 0.5626, time 156.61ms, mfu 3.12%\n",
            "iter 2400: loss 0.6482, time 159.83ms, mfu 3.13%\n",
            "iter 2410: loss 0.7048, time 158.53ms, mfu 3.14%\n",
            "iter 2420: loss 0.5917, time 157.83ms, mfu 3.15%\n",
            "iter 2430: loss 0.5594, time 157.67ms, mfu 3.15%\n",
            "iter 2440: loss 0.6353, time 159.40ms, mfu 3.16%\n",
            "iter 2450: loss 0.6355, time 158.56ms, mfu 3.16%\n",
            "iter 2460: loss 0.6375, time 159.87ms, mfu 3.17%\n",
            "iter 2470: loss 0.5999, time 157.07ms, mfu 3.17%\n",
            "iter 2480: loss 0.5711, time 158.69ms, mfu 3.18%\n",
            "iter 2490: loss 0.6229, time 159.48ms, mfu 3.18%\n",
            "step 2500: train loss 0.2755, val loss 8.1191\n",
            "saving checkpoint to out\n",
            "iter 2500: loss 0.5532, time 16454.35ms, mfu 2.86%\n",
            "iter 2510: loss 0.6323, time 157.78ms, mfu 2.90%\n",
            "iter 2520: loss 0.5863, time 160.26ms, mfu 2.93%\n",
            "iter 2530: loss 0.6276, time 158.28ms, mfu 2.96%\n",
            "iter 2540: loss 0.5467, time 157.62ms, mfu 2.98%\n",
            "iter 2550: loss 0.5426, time 157.00ms, mfu 3.01%\n",
            "iter 2560: loss 0.5215, time 159.71ms, mfu 3.03%\n",
            "iter 2570: loss 0.5810, time 159.29ms, mfu 3.04%\n",
            "iter 2580: loss 0.6166, time 159.16ms, mfu 3.06%\n",
            "iter 2590: loss 0.5516, time 156.44ms, mfu 3.08%\n",
            "iter 2600: loss 0.5746, time 158.33ms, mfu 3.09%\n",
            "iter 2610: loss 0.5261, time 158.91ms, mfu 3.10%\n",
            "iter 2620: loss 0.5471, time 157.19ms, mfu 3.12%\n",
            "iter 2630: loss 0.5338, time 161.05ms, mfu 3.12%\n",
            "iter 2640: loss 0.4668, time 159.27ms, mfu 3.13%\n",
            "iter 2650: loss 0.5204, time 157.44ms, mfu 3.14%\n",
            "iter 2660: loss 0.5244, time 160.30ms, mfu 3.14%\n",
            "iter 2670: loss 0.5356, time 159.46ms, mfu 3.15%\n",
            "iter 2680: loss 0.4884, time 159.44ms, mfu 3.15%\n",
            "iter 2690: loss 0.4749, time 158.90ms, mfu 3.16%\n",
            "iter 2700: loss 0.4973, time 157.18ms, mfu 3.17%\n",
            "iter 2710: loss 0.5034, time 157.70ms, mfu 3.17%\n",
            "iter 2720: loss 0.4857, time 156.57ms, mfu 3.18%\n",
            "iter 2730: loss 0.4934, time 160.08ms, mfu 3.18%\n",
            "iter 2740: loss 0.5275, time 158.86ms, mfu 3.18%\n",
            "step 2750: train loss 0.2137, val loss 8.2778\n",
            "saving checkpoint to out\n",
            "iter 2750: loss 0.4467, time 16924.28ms, mfu 2.87%\n",
            "iter 2760: loss 0.4974, time 161.56ms, mfu 2.90%\n",
            "iter 2770: loss 0.4963, time 157.17ms, mfu 2.93%\n",
            "iter 2780: loss 0.5007, time 160.23ms, mfu 2.95%\n",
            "iter 2790: loss 0.4448, time 157.16ms, mfu 2.98%\n",
            "iter 2800: loss 0.4825, time 158.81ms, mfu 3.01%\n",
            "iter 2810: loss 0.4525, time 159.49ms, mfu 3.02%\n",
            "iter 2820: loss 0.5127, time 158.93ms, mfu 3.04%\n",
            "iter 2830: loss 0.4690, time 160.54ms, mfu 3.06%\n",
            "iter 2840: loss 0.4608, time 157.16ms, mfu 3.07%\n",
            "iter 2850: loss 0.4517, time 156.22ms, mfu 3.09%\n",
            "iter 2860: loss 0.4507, time 158.44ms, mfu 3.10%\n",
            "iter 2870: loss 0.4723, time 157.30ms, mfu 3.12%\n",
            "iter 2880: loss 0.4668, time 159.40ms, mfu 3.13%\n",
            "iter 2890: loss 0.5342, time 160.89ms, mfu 3.13%\n",
            "iter 2900: loss 0.4447, time 160.25ms, mfu 3.13%\n",
            "iter 2910: loss 0.4380, time 160.45ms, mfu 3.14%\n",
            "iter 2920: loss 0.4790, time 158.95ms, mfu 3.14%\n",
            "iter 2930: loss 0.4110, time 160.13ms, mfu 3.15%\n",
            "iter 2940: loss 0.4283, time 159.59ms, mfu 3.15%\n",
            "iter 2950: loss 0.4263, time 158.77ms, mfu 3.16%\n",
            "iter 2960: loss 0.4038, time 161.54ms, mfu 3.16%\n",
            "iter 2970: loss 0.4331, time 157.96ms, mfu 3.16%\n",
            "iter 2980: loss 0.4145, time 157.37ms, mfu 3.17%\n",
            "iter 2990: loss 0.4122, time 160.57ms, mfu 3.17%\n",
            "step 3000: train loss 0.1678, val loss 8.4954\n",
            "saving checkpoint to out\n",
            "iter 3000: loss 0.4291, time 16361.49ms, mfu 2.86%\n",
            "iter 3010: loss 0.3653, time 156.77ms, mfu 2.90%\n",
            "iter 3020: loss 0.3861, time 158.02ms, mfu 2.93%\n",
            "iter 3030: loss 0.3831, time 157.41ms, mfu 2.96%\n",
            "iter 3040: loss 0.4421, time 160.98ms, mfu 2.98%\n",
            "iter 3050: loss 0.3776, time 156.85ms, mfu 3.01%\n",
            "iter 3060: loss 0.4103, time 158.89ms, mfu 3.03%\n",
            "iter 3070: loss 0.3852, time 156.85ms, mfu 3.05%\n",
            "iter 3080: loss 0.3837, time 159.75ms, mfu 3.06%\n",
            "iter 3090: loss 0.3937, time 160.05ms, mfu 3.07%\n",
            "iter 3100: loss 0.4151, time 159.55ms, mfu 3.09%\n",
            "iter 3110: loss 0.3837, time 158.72ms, mfu 3.10%\n",
            "iter 3120: loss 0.3823, time 158.55ms, mfu 3.11%\n",
            "iter 3130: loss 0.3697, time 159.78ms, mfu 3.12%\n",
            "iter 3140: loss 0.3988, time 158.15ms, mfu 3.13%\n",
            "iter 3150: loss 0.3786, time 160.35ms, mfu 3.13%\n",
            "iter 3160: loss 0.3904, time 157.91ms, mfu 3.14%\n",
            "iter 3170: loss 0.3820, time 161.16ms, mfu 3.14%\n",
            "iter 3180: loss 0.3424, time 156.47ms, mfu 3.15%\n",
            "iter 3190: loss 0.3644, time 158.34ms, mfu 3.16%\n",
            "iter 3200: loss 0.3813, time 159.30ms, mfu 3.16%\n",
            "iter 3210: loss 0.3616, time 156.59ms, mfu 3.17%\n",
            "iter 3220: loss 0.4148, time 157.84ms, mfu 3.18%\n",
            "iter 3230: loss 0.3513, time 159.80ms, mfu 3.18%\n",
            "iter 3240: loss 0.3832, time 160.91ms, mfu 3.18%\n",
            "step 3250: train loss 0.1400, val loss 8.6271\n",
            "saving checkpoint to out\n",
            "iter 3250: loss 0.3559, time 16494.85ms, mfu 2.86%\n",
            "iter 3260: loss 0.3556, time 158.84ms, mfu 2.90%\n",
            "iter 3270: loss 0.3600, time 158.22ms, mfu 2.93%\n",
            "iter 3280: loss 0.3376, time 156.37ms, mfu 2.96%\n",
            "iter 3290: loss 0.3788, time 158.01ms, mfu 2.99%\n",
            "iter 3300: loss 0.3277, time 156.68ms, mfu 3.01%\n",
            "iter 3310: loss 0.3271, time 157.61ms, mfu 3.04%\n",
            "iter 3320: loss 0.3666, time 159.75ms, mfu 3.05%\n",
            "iter 3330: loss 0.3370, time 158.33ms, mfu 3.07%\n",
            "iter 3340: loss 0.3417, time 158.92ms, mfu 3.08%\n",
            "iter 3350: loss 0.3154, time 159.45ms, mfu 3.09%\n",
            "iter 3360: loss 0.3235, time 156.49ms, mfu 3.11%\n",
            "iter 3370: loss 0.3542, time 158.13ms, mfu 3.12%\n",
            "iter 3380: loss 0.3525, time 158.49ms, mfu 3.13%\n",
            "iter 3390: loss 0.3463, time 159.31ms, mfu 3.14%\n",
            "iter 3400: loss 0.2959, time 158.65ms, mfu 3.14%\n",
            "iter 3410: loss 0.3108, time 155.69ms, mfu 3.16%\n",
            "iter 3420: loss 0.3482, time 160.50ms, mfu 3.16%\n",
            "iter 3430: loss 0.3181, time 158.84ms, mfu 3.16%\n",
            "iter 3440: loss 0.3010, time 158.17ms, mfu 3.17%\n",
            "iter 3450: loss 0.3137, time 161.38ms, mfu 3.17%\n",
            "iter 3460: loss 0.3164, time 160.18ms, mfu 3.17%\n",
            "iter 3470: loss 0.2743, time 159.86ms, mfu 3.17%\n",
            "iter 3480: loss 0.3261, time 155.80ms, mfu 3.18%\n",
            "iter 3490: loss 0.3141, time 161.92ms, mfu 3.18%\n",
            "step 3500: train loss 0.1153, val loss 8.7861\n",
            "saving checkpoint to out\n",
            "iter 3500: loss 0.2948, time 16500.38ms, mfu 2.86%\n",
            "iter 3510: loss 0.3151, time 161.21ms, mfu 2.89%\n",
            "iter 3520: loss 0.2947, time 158.29ms, mfu 2.92%\n",
            "iter 3530: loss 0.2977, time 159.55ms, mfu 2.95%\n",
            "iter 3540: loss 0.3115, time 160.29ms, mfu 2.97%\n",
            "iter 3550: loss 0.2810, time 158.91ms, mfu 3.00%\n",
            "iter 3560: loss 0.2961, time 159.23ms, mfu 3.02%\n",
            "iter 3570: loss 0.2994, time 157.54ms, mfu 3.04%\n",
            "iter 3580: loss 0.2862, time 160.10ms, mfu 3.05%\n",
            "iter 3590: loss 0.3154, time 159.21ms, mfu 3.07%\n",
            "iter 3600: loss 0.2689, time 158.60ms, mfu 3.08%\n",
            "iter 3610: loss 0.2974, time 160.05ms, mfu 3.09%\n",
            "iter 3620: loss 0.3008, time 159.20ms, mfu 3.10%\n",
            "iter 3630: loss 0.2976, time 158.96ms, mfu 3.11%\n",
            "iter 3640: loss 0.2724, time 159.77ms, mfu 3.12%\n",
            "iter 3650: loss 0.2923, time 159.13ms, mfu 3.13%\n",
            "iter 3660: loss 0.2732, time 158.87ms, mfu 3.13%\n",
            "iter 3670: loss 0.2940, time 160.39ms, mfu 3.14%\n",
            "iter 3680: loss 0.2886, time 160.65ms, mfu 3.14%\n",
            "iter 3690: loss 0.2787, time 160.55ms, mfu 3.14%\n",
            "iter 3700: loss 0.2926, time 156.95ms, mfu 3.15%\n",
            "iter 3710: loss 0.2828, time 157.75ms, mfu 3.16%\n",
            "iter 3720: loss 0.2555, time 158.13ms, mfu 3.17%\n",
            "iter 3730: loss 0.2614, time 158.96ms, mfu 3.17%\n",
            "iter 3740: loss 0.2642, time 158.65ms, mfu 3.18%\n",
            "step 3750: train loss 0.1008, val loss 8.8914\n",
            "saving checkpoint to out\n",
            "iter 3750: loss 0.2734, time 16353.61ms, mfu 2.86%\n",
            "iter 3760: loss 0.2536, time 159.10ms, mfu 2.89%\n",
            "iter 3770: loss 0.2521, time 158.50ms, mfu 2.93%\n",
            "iter 3780: loss 0.2433, time 159.28ms, mfu 2.95%\n",
            "iter 3790: loss 0.2502, time 160.46ms, mfu 2.98%\n",
            "iter 3800: loss 0.2575, time 159.99ms, mfu 3.00%\n",
            "iter 3810: loss 0.2696, time 157.81ms, mfu 3.02%\n",
            "iter 3820: loss 0.2782, time 158.14ms, mfu 3.04%\n",
            "iter 3830: loss 0.2591, time 157.76ms, mfu 3.06%\n",
            "iter 3840: loss 0.2605, time 158.64ms, mfu 3.07%\n",
            "iter 3850: loss 0.2632, time 161.39ms, mfu 3.08%\n",
            "iter 3860: loss 0.2336, time 158.04ms, mfu 3.10%\n",
            "iter 3870: loss 0.2704, time 157.41ms, mfu 3.11%\n",
            "iter 3880: loss 0.2532, time 159.17ms, mfu 3.12%\n",
            "iter 3890: loss 0.2648, time 159.30ms, mfu 3.13%\n",
            "iter 3900: loss 0.2307, time 159.15ms, mfu 3.13%\n",
            "iter 3910: loss 0.2512, time 157.72ms, mfu 3.14%\n",
            "iter 3920: loss 0.2415, time 157.08ms, mfu 3.15%\n",
            "iter 3930: loss 0.2402, time 159.58ms, mfu 3.16%\n",
            "iter 3940: loss 0.3039, time 159.79ms, mfu 3.16%\n",
            "iter 3950: loss 0.2388, time 158.51ms, mfu 3.16%\n",
            "iter 3960: loss 0.2513, time 157.93ms, mfu 3.17%\n",
            "iter 3970: loss 0.2321, time 159.95ms, mfu 3.17%\n",
            "iter 3980: loss 0.2528, time 161.23ms, mfu 3.17%\n",
            "iter 3990: loss 0.2414, time 157.45ms, mfu 3.18%\n",
            "step 4000: train loss 0.0890, val loss 8.9386\n",
            "saving checkpoint to out\n",
            "iter 4000: loss 0.2297, time 16597.44ms, mfu 2.86%\n",
            "iter 4010: loss 0.2477, time 159.99ms, mfu 2.89%\n",
            "iter 4020: loss 0.2115, time 157.10ms, mfu 2.93%\n",
            "iter 4030: loss 0.2391, time 157.14ms, mfu 2.96%\n",
            "iter 4040: loss 0.2353, time 158.11ms, mfu 2.99%\n",
            "iter 4050: loss 0.2424, time 159.38ms, mfu 3.01%\n",
            "iter 4060: loss 0.2329, time 156.32ms, mfu 3.03%\n",
            "iter 4070: loss 0.2443, time 158.22ms, mfu 3.05%\n",
            "iter 4080: loss 0.2382, time 159.05ms, mfu 3.07%\n",
            "iter 4090: loss 0.2283, time 159.38ms, mfu 3.08%\n",
            "iter 4100: loss 0.2539, time 159.79ms, mfu 3.09%\n",
            "iter 4110: loss 0.2065, time 160.23ms, mfu 3.10%\n",
            "iter 4120: loss 0.2484, time 159.10ms, mfu 3.11%\n",
            "iter 4130: loss 0.2362, time 159.27ms, mfu 3.12%\n",
            "iter 4140: loss 0.2359, time 157.42ms, mfu 3.13%\n",
            "iter 4150: loss 0.2080, time 158.78ms, mfu 3.14%\n",
            "iter 4160: loss 0.2087, time 159.39ms, mfu 3.14%\n",
            "iter 4170: loss 0.2169, time 159.08ms, mfu 3.15%\n",
            "iter 4180: loss 0.2122, time 156.40ms, mfu 3.16%\n",
            "iter 4190: loss 0.1997, time 159.16ms, mfu 3.16%\n",
            "iter 4200: loss 0.2122, time 159.94ms, mfu 3.16%\n",
            "iter 4210: loss 0.2350, time 157.89ms, mfu 3.17%\n",
            "iter 4220: loss 0.2093, time 160.11ms, mfu 3.17%\n",
            "iter 4230: loss 0.2039, time 159.29ms, mfu 3.17%\n",
            "iter 4240: loss 0.2093, time 160.39ms, mfu 3.17%\n",
            "step 4250: train loss 0.0823, val loss 8.9712\n",
            "saving checkpoint to out\n",
            "iter 4250: loss 0.2110, time 16385.72ms, mfu 2.86%\n",
            "iter 4260: loss 0.2150, time 155.56ms, mfu 2.90%\n",
            "iter 4270: loss 0.2074, time 158.68ms, mfu 2.93%\n",
            "iter 4280: loss 0.2170, time 157.32ms, mfu 2.96%\n",
            "iter 4290: loss 0.2281, time 158.50ms, mfu 2.99%\n",
            "iter 4300: loss 0.2091, time 157.74ms, mfu 3.01%\n",
            "iter 4310: loss 0.1993, time 158.58ms, mfu 3.03%\n",
            "iter 4320: loss 0.2150, time 158.97ms, mfu 3.05%\n",
            "iter 4330: loss 0.2138, time 159.33ms, mfu 3.06%\n",
            "iter 4340: loss 0.2068, time 161.07ms, mfu 3.07%\n",
            "iter 4350: loss 0.1976, time 157.16ms, mfu 3.09%\n",
            "iter 4360: loss 0.2333, time 157.94ms, mfu 3.10%\n",
            "iter 4370: loss 0.1958, time 159.89ms, mfu 3.11%\n",
            "iter 4380: loss 0.2136, time 158.26ms, mfu 3.12%\n",
            "iter 4390: loss 0.2147, time 158.05ms, mfu 3.13%\n",
            "iter 4400: loss 0.2062, time 157.34ms, mfu 3.14%\n",
            "iter 4410: loss 0.1898, time 157.44ms, mfu 3.15%\n",
            "iter 4420: loss 0.2124, time 160.37ms, mfu 3.15%\n",
            "iter 4430: loss 0.2058, time 160.64ms, mfu 3.16%\n",
            "iter 4440: loss 0.2081, time 158.74ms, mfu 3.16%\n",
            "iter 4450: loss 0.2068, time 157.90ms, mfu 3.17%\n",
            "iter 4460: loss 0.2176, time 159.31ms, mfu 3.17%\n",
            "iter 4470: loss 0.2161, time 159.89ms, mfu 3.17%\n",
            "iter 4480: loss 0.1998, time 159.23ms, mfu 3.17%\n",
            "iter 4490: loss 0.1735, time 157.85ms, mfu 3.18%\n",
            "step 4500: train loss 0.0760, val loss 9.0958\n",
            "saving checkpoint to out\n",
            "iter 4500: loss 0.2098, time 16538.72ms, mfu 2.86%\n",
            "iter 4510: loss 0.1880, time 156.84ms, mfu 2.90%\n",
            "iter 4520: loss 0.1866, time 158.30ms, mfu 2.93%\n",
            "iter 4530: loss 0.2003, time 157.05ms, mfu 2.96%\n",
            "iter 4540: loss 0.1908, time 158.07ms, mfu 2.99%\n",
            "iter 4550: loss 0.1860, time 158.68ms, mfu 3.01%\n",
            "iter 4560: loss 0.1960, time 158.39ms, mfu 3.03%\n",
            "iter 4570: loss 0.1741, time 159.45ms, mfu 3.05%\n",
            "iter 4580: loss 0.2018, time 159.46ms, mfu 3.06%\n",
            "iter 4590: loss 0.1913, time 160.81ms, mfu 3.07%\n",
            "iter 4600: loss 0.2193, time 160.88ms, mfu 3.08%\n",
            "iter 4610: loss 0.2003, time 159.79ms, mfu 3.09%\n",
            "iter 4620: loss 0.1812, time 159.84ms, mfu 3.10%\n",
            "iter 4630: loss 0.2061, time 159.82ms, mfu 3.11%\n",
            "iter 4640: loss 0.1743, time 159.70ms, mfu 3.12%\n",
            "iter 4650: loss 0.1934, time 158.65ms, mfu 3.13%\n",
            "iter 4660: loss 0.1764, time 158.75ms, mfu 3.14%\n",
            "iter 4670: loss 0.2100, time 159.72ms, mfu 3.14%\n",
            "iter 4680: loss 0.1952, time 159.80ms, mfu 3.14%\n",
            "iter 4690: loss 0.1927, time 160.25ms, mfu 3.15%\n",
            "iter 4700: loss 0.1820, time 157.00ms, mfu 3.16%\n",
            "iter 4710: loss 0.1819, time 158.29ms, mfu 3.16%\n",
            "iter 4720: loss 0.1964, time 158.28ms, mfu 3.17%\n",
            "iter 4730: loss 0.1959, time 158.87ms, mfu 3.17%\n",
            "iter 4740: loss 0.1740, time 158.78ms, mfu 3.18%\n",
            "step 4750: train loss 0.0748, val loss 9.0943\n",
            "saving checkpoint to out\n",
            "iter 4750: loss 0.1827, time 16329.83ms, mfu 2.86%\n",
            "iter 4760: loss 0.1890, time 160.02ms, mfu 2.89%\n",
            "iter 4770: loss 0.1929, time 158.84ms, mfu 2.92%\n",
            "iter 4780: loss 0.1819, time 158.34ms, mfu 2.95%\n",
            "iter 4790: loss 0.1917, time 158.75ms, mfu 2.98%\n",
            "iter 4800: loss 0.1686, time 158.64ms, mfu 3.00%\n",
            "iter 4810: loss 0.1918, time 157.42ms, mfu 3.03%\n",
            "iter 4820: loss 0.1899, time 158.70ms, mfu 3.04%\n",
            "iter 4830: loss 0.1695, time 158.28ms, mfu 3.06%\n",
            "iter 4840: loss 0.1762, time 156.73ms, mfu 3.08%\n",
            "iter 4850: loss 0.1709, time 158.26ms, mfu 3.09%\n",
            "iter 4860: loss 0.1707, time 159.60ms, mfu 3.10%\n",
            "iter 4870: loss 0.1982, time 157.83ms, mfu 3.12%\n",
            "iter 4880: loss 0.1899, time 157.59ms, mfu 3.13%\n",
            "iter 4890: loss 0.1860, time 156.59ms, mfu 3.14%\n",
            "iter 4900: loss 0.1791, time 157.46ms, mfu 3.15%\n",
            "iter 4910: loss 0.1835, time 159.02ms, mfu 3.15%\n",
            "iter 4920: loss 0.1793, time 157.63ms, mfu 3.16%\n",
            "iter 4930: loss 0.1727, time 158.45ms, mfu 3.17%\n",
            "iter 4940: loss 0.1865, time 156.53ms, mfu 3.18%\n",
            "iter 4950: loss 0.1553, time 157.41ms, mfu 3.18%\n",
            "iter 4960: loss 0.1755, time 158.02ms, mfu 3.19%\n",
            "iter 4970: loss 0.1589, time 158.38ms, mfu 3.19%\n",
            "iter 4980: loss 0.1691, time 159.67ms, mfu 3.19%\n",
            "iter 4990: loss 0.1820, time 158.33ms, mfu 3.19%\n",
            "step 5000: train loss 0.0722, val loss 9.1578\n",
            "saving checkpoint to out\n",
            "iter 5000: loss 0.1770, time 16493.64ms, mfu 2.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Outputs with our New Model\n",
        "\n",
        "Now we can leverage the `sample.py` file to generate outputs from our model!"
      ],
      "metadata": {
        "id": "L2J5JlRxFJOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Set Up and Model Loading"
      ],
      "metadata": {
        "id": "eo_QP1ITFfX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRB3j7iiNkl",
        "outputId": "5aa39cd1-f333-47e8-bf6e-f6e70d5c587f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29.17M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ],
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation!"
      ],
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTcaHCjii5l",
        "outputId": "d703e754-65ca-478d-de39-08b41cca90f7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To the tumbling billows of the main.\n",
            "Lord, Lord! methought, what pain it was to drown!\n",
            "What dreadful noise of waters in mine ears!\n",
            "What dreadful noise of waters in mine ears!\n",
            "What ugly sights of death within mine eyes!\n",
            "Methought I saw a thousand fearful wrecks;\n",
            "Ten thousand men that fishes gnaw'd upon;\n",
            "Wedges of gold, great anchors, heaps of pearl,\n",
            "Inestimable stones, unvalued jewels,\n",
            "All scatter'd in the bottom of the sea:\n",
            "Some lay in dead men's skulls; and, in those holes\n",
            "Where eyes did once inhabit, there were crept,\n",
            "As 'twere in scorn of eyes, reflecting gems,\n",
            "Which woo'd the slimy bottom of the deep,\n",
            "And mock'd the dead bones that lay scatter'd by.\n",
            "\n",
            "BRAKENBURY:\n",
            "Had you such leisure in the time of death\n",
            "To gaze upon the secrets of the deep?\n",
            "\n",
            "CLARENCE:\n",
            "Methought I had; and often did I strive\n",
            "To yield the ghost: but still the envious flood\n",
            "Kept in my soul, and would not let it forth\n",
            "To seek the empty, vast and wandering air;\n",
            "But smother'd it within my panting bulk,\n",
            "Which almost burst to belch it in the sea.\n",
            "\n",
            "BRAKENBURY:\n",
            "Awaked you not with this sore agony?\n",
            "\n",
            "CLARENCE:\n",
            "O, no, my dream was lengthen'd after life;\n",
            "O, then began the tempest to my soul,\n",
            "Who pass'd, methought, the melancholy flood,\n",
            "With that grim ferryman which poets write of,\n",
            "Unto the kingdom of perpetual night.\n",
            "The first that there did greet my stranger soul,\n",
            "Was my great father-in-law, renowned Warwick;\n",
            "Who cried aloud, 'What scourge for perjury\n",
            "Can this dark monarchy afford false Clarence?'\n",
            "And so he vanish'd: then came wandering by\n",
            "A shadow like an angel, with bright hair\n",
            "Dabbled in blood; and he squeak'd out aloud,\n",
            "'Clarence is come; false, fleeting, perjured Clarence,\n",
            "That stabb'd me in the field by Tewksbury;\n",
            "Seize on him, Furies, take him to your torments!'\n",
            "With that, methoughts, a legion of foul fiends\n",
            "Environ'd me about, and howled\n",
            "---------------\n",
            "\n",
            "\n",
            "KING EDWARD IV:\n",
            "Now, brother of this night's will and hand;\n",
            "And, love the way that once more I want of love;\n",
            "And, love to her, her,\n",
            "Since that I have already.\n",
            "\n",
            "LADY GREY:\n",
            "To tell you plain, I had rather lie in prison.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Why, then, Buckingham, do you father's lands.\n",
            "\n",
            "LADY GREY:\n",
            "To do them good, I would sustain some harm.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Then get your husband's lands, to do them good.\n",
            "\n",
            "LADY GREY:\n",
            "Therefore I came unto your majesty.\n",
            "\n",
            "KING EDWARD IV:\n",
            "I'll tell you how these lands are to be got.\n",
            "\n",
            "LADY GREY:\n",
            "So shall you bind me to your highness' service.\n",
            "\n",
            "KING EDWARD IV:\n",
            "What service wilt thou do me, if I give them?\n",
            "\n",
            "LADY GREY:\n",
            "What you command, that rests in me to do.\n",
            "\n",
            "KING EDWARD IV:\n",
            "But you will take exceptions to my boon.\n",
            "\n",
            "LADY GREY:\n",
            "No, gracious lord, except I cannot do it.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Ay, but thou canst do what I mean to ask.\n",
            "\n",
            "LADY GREY:\n",
            "Why, then I will do what your grace commands.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "CLARENCE:\n",
            "\n",
            "LADY GREY:\n",
            "Why stops my lord, shall I not hear my task?\n",
            "\n",
            "KING EDWARD IV:\n",
            "An easy task; 'tis but to love a king.\n",
            "\n",
            "LADY GREY:\n",
            "That's soon perform'd, because I am a subject.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "KING EDWARD IV:\n",
            "But stay thee, 'tis the fruits of love I mean.\n",
            "\n",
            "LADY GREY:\n",
            "The fruits of love I mean, my loving liege.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Ay, but, I fear me, in another sense.\n",
            "What love, think'st thou, I sue so much to get?\n",
            "\n",
            "LADY GREY:\n",
            "My love till death, my humble thanks, my prayers;\n",
            "That love which virtue begs and virtue grants.\n",
            "\n",
            "KING EDWARD IV:\n",
            "No, by my troth, I did not mean such love.\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "The one so dishonour to do't, but that remorse\n",
            "He's life?\n",
            "\n",
            "ANGELO:\n",
            "I show it, and, if you give me love.\n",
            "\n",
            "ISABELLA:\n",
            "I will not, were it then.\n",
            "\n",
            "ANGELO:\n",
            "Look, if you do't, that he do't,\n",
            "ISABELLA:\n",
            "But might you would? that I cannot do.\n",
            "\n",
            "ANGELO:\n",
            "Look, what I will not, that I cannot do.\n",
            "\n",
            "ISABELLA:\n",
            "But might you do't, and do the world no wrong,\n",
            "If so your heart were touch'd with that remorse\n",
            "As mine is to him?\n",
            "\n",
            "ANGELO:\n",
            "He's sentenced; 'tis too late.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA:\n",
            "Too late? why, no; I, that do speak a word.\n",
            "May call it back again. Well, believe this,\n",
            "No ceremony that to great ones 'longs,\n",
            "Not the king's crown, nor the deputed sword,\n",
            "The marshal's truncheon, nor the judge's robe,\n",
            "Become them with one half so good a grace\n",
            "As mercy does.\n",
            "If he had been as you as he,\n",
            "You would have slipt like you, like him; but he, like you,\n",
            "Would not have been so stern.\n",
            "\n",
            "ANGELO:\n",
            "Pray you, be gone.\n",
            "\n",
            "ISABELLA:\n",
            "I would to heaven I had your potency,\n",
            "And you were Isabel! should it then be thus?\n",
            "No; I would tell what 'twere to be a judge,\n",
            "And what a prisoner.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ANGELO:\n",
            "Your brother is a forfeit of the law,\n",
            "And you but waste your words.\n",
            "\n",
            "ISABELLA:\n",
            "Alas, alas!\n",
            "Why, all the souls that were were were were were forfeit once;\n",
            "And He that might the vantage best have took\n",
            "Found out the remedy. How would you be,\n",
            "If He, which is the top of judgment, should\n",
            "But judge you as you are? O, think on that;\n",
            "And mercy then will breathe within your lips,\n",
            "Like man new made.\n",
            "\n",
            "ANGELO:\n",
            "Be you content, fair maid;\n",
            "---------------\n",
            "\n",
            "\n",
            "CORIOLANUS:\n",
            "Pray you, I'll straight do\n",
            "Stand to them?\n",
            "\n",
            "MENENIUS:\n",
            "Be calm, and you have my desire.\n",
            "\n",
            "CORIOLANUS:\n",
            "It then, I pray, I pray you,\n",
            "My brother, and prove you, price o' the consulship?\n",
            "\n",
            "First Citizen:\n",
            "The price is to ask it kindly.\n",
            "\n",
            "CORIOLANUS:\n",
            "Kindly! Sir, I pray, let me ha't: I have wounds to\n",
            "show you, which shall be yours in private. Your\n",
            "good voice, sir; what say you?\n",
            "\n",
            "Second Citizen:\n",
            "You shall ha' it, worthy sir.\n",
            "\n",
            "CORIOLANUS:\n",
            "A match, sir. There's in all two worthy voices\n",
            "begged. I have your alms: adieu.\n",
            "\n",
            "Third Citizen:\n",
            "But this is something odd.\n",
            "\n",
            "Second Citizen:\n",
            "An 'twere to give again,--but 'tis no matter.\n",
            "\n",
            "CORIOLANUS:\n",
            "Pray you now, if it may stand with the tune of your\n",
            "voices that I may be consul, I have here the\n",
            "customary gown.\n",
            "\n",
            "Fourth Citizen:\n",
            "You have deserved nobly of your country, and you\n",
            "have not deserved nobly.\n",
            "\n",
            "CORIOLANUS:\n",
            "Your enigma?\n",
            "\n",
            "Fourth Citizen:\n",
            "You have been a scourge to her enemies, you have\n",
            "been a rod to her friends; you have not indeed loved\n",
            "the common people.\n",
            "\n",
            "CORIOLANUS:\n",
            "You should account me the more virtuous that I have\n",
            "not been common in my love. I will, sir, flatter my\n",
            "sworn brother, the people, to earn a dearer\n",
            "estimation of them; 'tis a condition they account\n",
            "gentle: and since the wisdom of their choice is\n",
            "rather to have my heart, I will practise\n",
            "the insinuating nod and be off to them most\n",
            "counterfeitly; that is, sir, I will counterfeit the\n",
            "bewitchment of some popular man and give it\n",
            "bountiful to the desirers. Therefore, beseech you,\n",
            "I may be consul.\n",
            "\n",
            "Fifth Citizen:\n",
            "We hope to find you our friend; and therefore give\n",
            "you our voices heartily.\n",
            "\n",
            "Fourth Citizen:\n",
            "Y\n",
            "---------------\n",
            "\n",
            "\n",
            "JULIET:\n",
            "Ay, it is an evil, and give me strength,\n",
            "Of death, Romeo! O Romeo!\n",
            "A lightning, tell me of faith, tell me,\n",
            "In what news? I do remember\n",
            "By playing it to me with so sour a face.\n",
            "\n",
            "Nurse:\n",
            "I am not stay awhile:\n",
            "Thou art thyself, I thy form cries out a face.\n",
            "\n",
            "JULIET:\n",
            "Indeed, I see thee, gave thee--\n",
            "Alack, that thou hadst not a man in hell.\n",
            "\n",
            "Nurse:\n",
            "Alack, alack, that thou hadst not by thee from the world,\n",
            "That thou didst bower the mark of this world?\n",
            "\n",
            "JULIET:\n",
            "I know not how I, friar,\n",
            "Upon his name that thou art below,\n",
            "By playing it to me with so sour a face.\n",
            "\n",
            "Nurse:\n",
            "I am a thousand times, madam;\n",
            "Have I thy the prettiest babe,\n",
            "And yet I might live to see thee married once,\n",
            "And I might live to see thee married once,\n",
            "Because I might live, to Romeo and I dream of death!\n",
            "\n",
            "LADY CAPULET:\n",
            "Marry, he is a careful father,\n",
            "One who, to put thee married with a sudden!\n",
            "\n",
            "JULIET:\n",
            "When I might they, that I thine eyes?\n",
            "\n",
            "LADY CAPULET:\n",
            "That is, Romeo.\n",
            "\n",
            "JULIET:\n",
            "\n",
            "Ay, I shall be satisfied\n",
            "I have night, Romeo.\n",
            "\n",
            "LADY CAPULET:\n",
            "Alack, alack, it is Romeo!\n",
            "\n",
            "JULIET:\n",
            "What devil art thou, that dost torment me thus?\n",
            "This torture should be roar'd in dismal hell.\n",
            "Hath Romeo slain himself? say thou but 'I' shall poison more\n",
            "And that bare vowel 'I' shall poison more\n",
            "Than the death-darting eye of cockatrice:\n",
            "I am not I, if there be such an I;\n",
            "Or those eyes shut, that make thee answer 'I.'\n",
            "If he be slain, say 'I'; or if not, no:\n",
            "Brief sounds determine of my weal or woe.\n",
            "\n",
            "Nurse:\n",
            "I saw the wound, I saw it with mine eyes,--\n",
            "God save the mark!--here on his manly breast:\n",
            "A\n",
            "---------------\n",
            "\n",
            "I am a bush,\n",
            "That I shall have a bush,\n",
            "Whose bright wings misdoubteth every bush;\n",
            "And I, the hapless male to one sweet bird,\n",
            "Have now the fatal object in my eye\n",
            "Where my poor young was limed, was caught and kill'd.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, what a peevish fool was that of Crete,\n",
            "That taught his son the office of a fowl!\n",
            "An yet, for all his wings, the fool was drown'd.\n",
            "\n",
            "KING HENRY VI:\n",
            "I, Daedalus; my poor boy, Icarus;\n",
            "Thy father, Minos, that denied our course;\n",
            "Thy brother Edward, and thyself the wings of my sweet boy\n",
            "Thy brother Edward, and thyself the sea\n",
            "Whose envious gulf did swallow up his life.\n",
            "Ah, kill me with thy weapon, not with words!\n",
            "My breast can better brook thy dagger's point\n",
            "Than can my ears that tragic history.\n",
            "But wherefore dost thou come? is't for my life?\n",
            "\n",
            "GLOUCESTER:\n",
            "Think'st thou I am an executioner?\n",
            "\n",
            "KING HENRY VI:\n",
            "A persecutor, I am sure, thou art:\n",
            "If murdering innocents be executing,\n",
            "Why, then thou art an executioner.\n",
            "\n",
            "GLOUCESTER:\n",
            "Thy son I kill'd for his presumption.\n",
            "\n",
            "KING HENRY VI:\n",
            "Hadst thou been kill'd when first thou didst presume,\n",
            "Thou hadst not lived to kill a son of mine.\n",
            "And thus I prophesy, that many a thousand,\n",
            "Which now mistrust no parcel of my fear,\n",
            "And many an old man's sigh and many a widow's,\n",
            "And many an orphan's water-standing eye--\n",
            "Men for their sons, wives for their husbands,\n",
            "And orphans for their parents timeless death--\n",
            "Shall rue the hour that ever thou wast born.\n",
            "The owl shriek'd at thy birth,--an evil sign;\n",
            "The night-crow cried, aboding luckless time;\n",
            "Dogs howl'd, and hideous tempest shook down trees;\n",
            "The raven rook'd her on the chimney's top,\n",
            "And chattering pies in dismal discords sung.\n",
            "Thy mother felt more than a mother's pain,\n",
            "And, yet brought forth less than a mother's hope,\n",
            "To wit, an indigested and deformed lump,\n",
            "Not like the fruit\n",
            "---------------\n",
            "\n",
            "The slave of the devil'st degree\n",
            "Didst thou acquit thee. Thou art amazed.\n",
            "O, O hateful deeds committed\n",
            "Which didst bower the spirit hath forced to revenge!\n",
            "Canst thou art amazed; and I,\n",
            "For doing damned hate thyself.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Who lives, by God is here, and repent me,\n",
            "If thou think'st, not thy speech.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "By heaven, I was Barnardine,\n",
            "Which is touch'd, after after after gall'd,\n",
            "I'Tis meet him here to him.\n",
            "\n",
            "LUCIO:\n",
            "I think thou wert so,\n",
            "I was a traitor for his grace.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This is no other medicine\n",
            "But,, as it is true,\n",
            "To use it is true.\n",
            "\n",
            "ANGELO:\n",
            "My lord, I fear;\n",
            "I humbly thank you, now for that I come to demand\n",
            "And so your pleasure.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This is your companion by day.\n",
            "\n",
            "ISABELLA:\n",
            "Hark, I'll perfect.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "This is a story ready for your knowledge\n",
            "To enter publicly: volumes of report\n",
            "Run with these false and most profound,\n",
            "That's hear me,--\n",
            "\n",
            "LUCIO:\n",
            "'Tis not at all;\n",
            "As if the messenger,--\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The duke's pardon. I pray'd,\n",
            "Away with me to see what time: speak.\n",
            "\n",
            "LUCIO:\n",
            "I warrant your honour.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The warrants for yourself; take heed to't.\n",
            "\n",
            "ISABELLA:\n",
            "This gentleman told somewhat of my tale,--\n",
            "\n",
            "LUCIO:\n",
            "Right.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "It may be right; but you are i' the wrong\n",
            "To speak before your time. Proceed.\n",
            "\n",
            "ISABELLA:\n",
            "To this pernicious caitiff deputy,--\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "That's somewhat madly spoken.\n",
            "\n",
            "ISABELLA:\n",
            "Pardon it;\n",
            "The phrase is to the matter.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Mended again. The matter; proceed.\n",
            "\n",
            "ISABELLA:\n",
            "In brief, to set the needless process by,\n",
            "How I persuaded, how I pray'd, and kneel'd,\n",
            "---------------\n",
            "\n",
            "I could make you better, when I can sol, fa, and sing.\n",
            "\n",
            "GRUMIO:\n",
            "Help, masters, help! my master is mad.\n",
            "\n",
            "PETRUCHIO:\n",
            "Now, knock when I bid you, sirrah villain!\n",
            "\n",
            "HORTENSIO:\n",
            "How now! what's the matter? My old friend Grumio!\n",
            "and my good friend Petruchio! How do you all at Verona?\n",
            "\n",
            "PETRUCHIO:\n",
            "Signior Hortensio, come you to part the fray?\n",
            "'Con tutto il cuore, ben trovato,' may I say.\n",
            "\n",
            "HORTENSIO:\n",
            "'Alla nostra casa ben venuto, molto honorato signor\n",
            "mio Petruchio.' Rise, Grumio, rise: we will compound\n",
            "this quarrel.\n",
            "\n",
            "GRUMIO:\n",
            "Nay, 'tis no matter, sir, what he 'leges in Latin.\n",
            "if this be not a lawful case for me to leave his\n",
            "service, look you, sir: well, he bid me knock him and rap\n",
            "use his master so, was it fit for aught I see,\n",
            "two and thirty, a pip out? Whom would to God I had\n",
            "well knock'd at first, Then had not Grumio come by the worst.\n",
            "\n",
            "PETRUCHIO:\n",
            "A senseless villain! Good Hortensio,\n",
            "I bade the rascal knock upon your gate\n",
            "And could not get him for my heart to do it.\n",
            "\n",
            "GRUMIO:\n",
            "Knock at the gate! O heavens! Spake you not these\n",
            "words plain, 'Sirrah, knock me here, rap me here,\n",
            "knock me well, and knock me soundly'? And come you\n",
            "now with, 'knocking at the gate'?\n",
            "\n",
            "PETRUCHIO:\n",
            "Sirrah, be gone, or talk not, I advise you.\n",
            "\n",
            "HORTENSIO:\n",
            "Petruchio, patience; I am Grumio's pledge:\n",
            "Why, this's a heavy chance 'twixt him and you,\n",
            "Your ancient, trusty, pleasant servant Grumio.\n",
            "And tell me now, sweet friend, what happy gale\n",
            "Blows you to Padua here from old Verona?\n",
            "\n",
            "PETRUCHIO:\n",
            "Such wind as scatters young men through the world,\n",
            "To seek their fortunes farther than at home\n",
            "Where small experience grows. But in a few,\n",
            "Signior Hortensio, thus it stands with me:\n",
            "Antonio,\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "Once more, that makes.\n",
            "\n",
            "PAULINA:\n",
            "I care not:\n",
            "Not so, good my lords, you seen this most sovereign sir,\n",
            "Can do't, and on mine own accord I'll dearer\n",
            "But first I'll do my errand. The good queen,\n",
            "For she is good, hath brought you forth a daughter;\n",
            "Here 'tis; commends it to your blessing.\n",
            "\n",
            "LEONTES:\n",
            "Out!\n",
            "A mankind witch! Hence with her, out o' door:\n",
            "A most intelligencing bawd!\n",
            "\n",
            "PAULINA:\n",
            "Not so:\n",
            "I am as ignorant in that as you\n",
            "In so entitling me, and no less honest\n",
            "Than you are mad; which is enough, I'll warrant,\n",
            "As this world goes, to pass for honest.\n",
            "\n",
            "LEONTES:\n",
            "Traitors!\n",
            "Will you not push her out? Give her the bastard.\n",
            "Thou dotard! thou art woman-tired, unroosted\n",
            "By thy dame Partlet here. Take up the bastard;\n",
            "Take't up, I say; give't to thy crone.\n",
            "\n",
            "PAULINA:\n",
            "For ever\n",
            "Unvenerable be thy hands, if thou\n",
            "Takest up the princess by that forced baseness\n",
            "Which he has put upon't!\n",
            "\n",
            "LEONTES:\n",
            "He dreads his wife.\n",
            "\n",
            "PAULINA:\n",
            "So I would you did; then 'twere past all doubt\n",
            "You'ld call your children yours.\n",
            "\n",
            "LEONTES:\n",
            "A nest of traitors!\n",
            "\n",
            "ANTIGONUS:\n",
            "I am none, by this good light.\n",
            "\n",
            "PAULINA:\n",
            "Nor I, nor any\n",
            "But one that's here, and that's himself, for he\n",
            "The sacred honour of himself, his queen's,\n",
            "His hopeful son's, his babe's, betrays to slander,\n",
            "Whose sting is sharper than the sword's;\n",
            "and will not--\n",
            "For, as the case now stands, it is a curse\n",
            "He cannot be compell'd to't--once remove\n",
            "The root of his opinion, which is rotten\n",
            "As ever oak or stone was sound.\n",
            "\n",
            "LEONTES:\n",
            "A callat\n",
            "Of boundless tongue, who late hath beat her husband\n",
            "And now baits me! This brat is none of mine;\n",
            "It is the issue of Polixenes:\n",
            "Hence\n",
            "---------------\n",
            "\n",
            "Not rash-violator;\n",
            "Is it is ten times strange and strange.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Nay, it is ten times strange.\n",
            "\n",
            "ISABELLA:\n",
            "It is not truer he is Angelo\n",
            "Than this is all as true as it is strange:\n",
            "Nay, it is ten times true; for truth is truth\n",
            "To the end of reckoning.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Away with her! Poor soul,\n",
            "She speaks this in the infirmity of sense.\n",
            "\n",
            "ISABELLA:\n",
            "O prince, I conjure thee, as thou believest\n",
            "There is another comfort than this world,\n",
            "That thou neglect me not, with that opinion\n",
            "That I am touch'd with madness! Make not impossible\n",
            "That which but seems unlike: 'tis not impossible\n",
            "But one, the wicked'st caitiff on the ground,\n",
            "May seem as shy, as grave, as grave, as absolute\n",
            "As Angelo; even so may Angelo,\n",
            "In all his dressings, characts, titles, titles, forms,\n",
            "Be an arch-villain; believe it, royal prince:\n",
            "If he be less, he's nothing; but he's more,\n",
            "Had I more name for badness.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "By mine honesty,\n",
            "If she be mad,--as I believe no other,--\n",
            "Her madness hath the oddest frame of sense,\n",
            "Such a dependency of thing on thing,\n",
            "As e'er I heard in madness.\n",
            "\n",
            "ISABELLA:\n",
            "O gracious duke,\n",
            "Harp not on that, nor do not banish reason\n",
            "For inequality; but let your reason serve\n",
            "To make the truth appear where it seems hid,\n",
            "And hide the false seems true.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Many that are not mad\n",
            "Have, more lack of reason. What would you say?\n",
            "\n",
            "ISABELLA:\n",
            "I am the sister of one Claudio,\n",
            "Condemn'd upon the act of fornication\n",
            "To lose his head; condemn'd by Angelo:\n",
            "I, in probation of a sisterhood,\n",
            "Was sent to by my brother; one Lucio\n",
            "As then the messenger,--\n",
            "\n",
            "LUCIO:\n",
            "That's I, an't like your grace:\n",
            "I came to her from Claudio, and desired her\n",
            "To try her gracious fortune with Lord Angelo\n",
            "For her poor brother's pardon\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}