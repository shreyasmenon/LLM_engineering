{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot\n",
        "\n",
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)\n",
        "\n",
        "Let's explore the world of Few-Shot prompting and all it can do!"
      ],
      "metadata": {
        "id": "UIuhLOcmCdyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai cohere tiktoken -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2y8NcRnD8aa",
        "outputId": "81b1478f-35ec-401a-e97c-171d01f49722"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Key"
      ],
      "metadata": {
        "id": "jwzSC6pJpORd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tpnsDCfEbsqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3635398c-aa52-4a9b-8d13-90e6770bf480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "# set the OPENAI_API_KEY environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "rQw5oRsAJKcm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Helper Functions\n",
        "\n",
        "These are only meant to display the outputs in a more palatable style - you can call the API directly using the functions as a guide."
      ],
      "metadata": {
        "id": "Btqi64IPpQHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's focus on extending this a bit, and incorporate a `system` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The system message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the system prompt."
      ],
      "metadata": {
        "id": "UPs3ScS1WpoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QSQMFfWKbsqT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(messages: list) -> str:\n",
        "  client = OpenAI()\n",
        "  response = client.chat.completions.create(\n",
        "      messages=messages,\n",
        "      model= \"gpt-4-1106-preview\",#\"gpt-3.5-turbo\",#,\n",
        "  )\n",
        "  del client\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "def wrap_prompt(message: str, role: str) -> dict:\n",
        "    return {\"role\": role, \"content\": message}\n",
        "\n",
        "def m_print(message: str) -> str:\n",
        "    display(Markdown(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if our model can define nonsense terms for us."
      ],
      "metadata": {
        "id": "3zg7PNgKTyni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7aEd_p1sbsqT",
        "outputId": "a46acc85-1468-4d08-fc9e-a8cc3266972b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As of my last update in April 2023, \"flimple-dimple\" is not a recognized term in any standard English language dictionaries, scientific terminology, or widely known slang. It appears to be a nonsensical phrase or a made-up word that could have been used in a specific context, such as within a particular community, game, or as part of a private joke.\n\nIf \"flimple-dimple\" has a meaning in a specific context or if it has been coined after my last update, I would not be aware of it. For accurate information, you might want to provide more context or check the most up-to-date sources available."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = wrap_prompt(\"\"\"What is a flimple-dimple?\"\"\", \"user\")\n",
        "\n",
        "m_print(get_response([prompt]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if adding a system prompt helps us."
      ],
      "metadata": {
        "id": "WcC5SCDoT8up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    wrap_prompt(\"You are a creative children's author.\", \"system\"),\n",
        "    wrap_prompt(\"What is a flimple-dimple?\", \"user\")\n",
        "]\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "7e5f4255-437b-4131-8d2a-4678458c971e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A flimple-dimple is a whimsical creature from the reaches of my creative imagination, ideal for a children's story. Let me introduce you to this delightful being:\n\nIn the far-off land of Giggleberry Grove, deep within the whispering Wizzlewood Forest, you might be lucky enough to spot a flimple-dimple if you're very quiet and very still. A flimple-dimple is a small, furry creature, not much larger than a bunny, with a plump, cushiony body that's perfect for cuddling. It has a tuft of shimmering feathers atop its head that change color with its mood: blue for happy, green for curious, and pink for utterly delighted.\n\nThe flimple-dimple has big, expressive eyes the color of dewdrop diamonds and a tiny, shiny nose that wiggles when it senses a giggle. Its ears are floppy and soft, perfect for listening to the secrets of the forest or the hushed whispers of children sharing stories.\n\nIt’s known for its peculiar feet, which have squishy pads and little tickle-toes that make a “flimple-dimple” sound when it walks – a sound that's almost like a giggle turned into rhythm. That's how this creature got its name!\n\nThe flimple-dimple’s diet is just as whimsical as it is, feeding on rainbow sprouts and nectar from the Laughing Lily, the only flower that tickles the flimple-dimple's nose right back as it eats!\n\nChildren are enchanted by flimple-dimples because of their gentle nature and their magical ability to sense the innocence and joy in a child’s heart. By night, the flimple-dimple spreads dreams that are spun from candyfloss and little bursts of starlight, ensuring children slumber in a cocoon of happy dreams.\n\nIf you ever find yourself in Wizzlewood Forest, remember to carry a pocket full of giggles and a heart full of wonder: you might just befriend a flimple-dimple of your very own."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is really quite excellent - but isn't necessarily correct.\n",
        "\n",
        "Let's try modifying our system prompt to see if anything changes."
      ],
      "metadata": {
        "id": "MHu56wTnUOQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts[0] = wrap_prompt(\"You are a creative poet.\", \"system\")\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "933b7bce-f747-4564-af00-b8e0455784a5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In a world where language twists and bends,\nA flimple-dimple's not a thing that tends to trends.\nInvented in the moment, sprightly and light,\nA flimple-dimple dances in fanciful flight.\n\nA flimple could be a flutter, a frill on a fish,\nA dimple, a ripple, an impish little wish.\nSurrounded by whimsy, it's a creature of glee,\nSomething so delightful, it giggles with glee.\n\n'Tis a frivolous trifle, a trinket of the mind,\nA whimsical riddle that's uniquely designed.\nIt could be the bounce in a jubilant step,\nOr the curve of a smile that's cheekily kept.\n\nIn a world of wonder, a flimple-dimple's free,\nTo be anything you wish, anything you see.\nSo let's raise our glasses, to the make-believe's simple,\nHere's to life's little joys, like the flimple-dimple!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is a cool and complete answer - what if we wanted to teach the model what a flimple-dimple is?"
      ],
      "metadata": {
        "id": "Ci7pAEGkO_gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ],
      "metadata": {
        "id": "eqMRJLbOYcwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    wrap_prompt(\"A 'flimple-dimple' is a magical and wonderful creature. An example of a sentence that uses the word 'flimple-dimple' is:\", \"user\"),\n",
        "    wrap_prompt(\"'Look at that marvelous flimple-dimple flying over there!'\", \"assistant\"),\n",
        "    wrap_prompt(\"To 'chillain' is to flit about rapidly in excitement. An example of a sentence that uses the words 'flimple-dimple' and 'chillain' is:\", \"user\")\n",
        "]\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "9bf26c79-6f14-4791-dc26-cb40bf36d213"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\"The flimple-dimple was chillaining through the mystical forest, its wings buzzing with joy as it darted between the ancient, enchanted trees.\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, leveraging the `assistant` role lets us teach our model new words, despite never needing to train the model.\n",
        "\n",
        "This is a (simple) example of \"In-Context Learning\"!"
      ],
      "metadata": {
        "id": "W0zn9-X2d23Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning New Tasks\n",
        "\n",
        "Now, we're not limited to just teaching our model silly words.\n",
        "\n",
        "Let's see that in action!"
      ],
      "metadata": {
        "id": "w26xNjO7WWQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    wrap_prompt(\"You provide appropriate sentiments for user's inputs and output them in a JSON object.\", \"user\"),\n",
        "    wrap_prompt(\"Man, I hate cheese!\", \"user\"),\n",
        "    wrap_prompt(\"{'sentiment' : 'negative'}\", \"assistant\"),\n",
        "    wrap_prompt(\"Oh boy, cheese is my favourite!\", \"user\"),\n",
        "    wrap_prompt(\"{'sentiment' : 'positive'}\", \"assistant\"),\n",
        "    wrap_prompt(\"Cheese is hella mid.\", \"user\"),\n",
        "    wrap_prompt(\"{'sentiment' : 'neutral'}\", \"assistant\"),\n",
        "    wrap_prompt(\"The Incredible Hulk is the best, and greatest superhero ever!\", \"user\"),\n",
        "]\n",
        "\n",
        "m_print(get_response(list_of_prompts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "an4OR-uyWp61",
        "outputId": "a3c1ba3b-2678-421e-c521-d5b8bedbf09b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "{\"sentiment\": \"positive\"}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how we're able to both instruct our model on the new task - as well as the output format all through prompting!"
      ],
      "metadata": {
        "id": "iGbR9Pr_XGKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Portion\n",
        "\n",
        "Use few-shot prompting to allow the model to achieve a task it's not trained to do!"
      ],
      "metadata": {
        "id": "QLZzABmNQFa0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EK2xbXFZQMuS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "open_ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}